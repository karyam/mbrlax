{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karyam/Library/Caches/pypoetry/virtualenvs/mbrlax-W83qhuLc-py3.8/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:1806: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in array is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax_internal._check_user_dtype_supported(dtype, \"array\")\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'policy_loss' from 'mbrlax.utils.loss_functions' (/Users/karyam/Desktop/mbrlax/mbrlax/utils/loss_functions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xf/9wlqrlfd1dz_z97m_qlvwy6w0000gn/T/ipykernel_1428/1418837959.py\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbsuite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcartpole\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmbrlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPModelSpec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVGP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialize_gp_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmbrlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReplayBuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnvironmentModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmbrlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/mbrlax/mbrlax/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m from mbrlax import (\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/mbrlax/mbrlax/agents/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmbrlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpilco_agent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPilcoAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/mbrlax/mbrlax/agents/pilco_agent.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmbrlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPTransitionModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmbrlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEpisodeMetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnvironmentModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReplayBuffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#TODO: meaningfully log training result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/mbrlax/mbrlax/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmbrlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msample_mvn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmbrlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'policy_loss' from 'mbrlax.utils.loss_functions' (/Users/karyam/Desktop/mbrlax/mbrlax/utils/loss_functions.py)"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from bsuite.environments import cartpole\n",
    "\n",
    "from mbrlax.models import GPModelSpec, SVGP, initialize_gp_model\n",
    "from mbrlax.utils import ReplayBuffer, Driver, EnvironmentModel\n",
    "from mbrlax.policy import GPPolicy\n",
    "from mbrlax.transition_model import GPTransitionModel\n",
    "from mbrlax.optimizers import SGD\n",
    "from mbrlax.utils.initial_state_model import ParticleInitialStateModel\n",
    "\n",
    "from gpjax.likelihoods import Gaussian\n",
    "from gpjax.parameters import build_constrain_params\n",
    "from gpjax.datasets import CustomDataset, NumpyLoader\n",
    "from gpjax.config import default_float\n",
    "\n",
    "from gpflow_pilco.envs import CartPole\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import tensorflow as tf\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(params):\n",
    "    return params[0] * 2 + params[1] * 3\n",
    "\n",
    "def nested_grad(blah, params):\n",
    "    def step(params, tmp):\n",
    "        res = func(params)\n",
    "        return params, res\n",
    "        \n",
    "    _, scan_out = jax.lax.scan(\n",
    "        step,\n",
    "        params,\n",
    "        jnp.zeros((3,))\n",
    "    )\n",
    "    return jnp.sum(scan_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48026035987734006\n",
      "0.48026035987734006\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "key1, key2 = jax.random.split(key)\n",
    "key3, key4 = jax.random.split(key)\n",
    "assert key2.all() == key4.all()\n",
    "x = jax.random.uniform(key1)\n",
    "y = jax.random.uniform(key3)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([6., 9.], dtype=float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = jnp.array([1.0,2.0])\n",
    "value, grad = jax.value_and_grad(nested_grad, argnums=1)(\"test\", params)\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = default_float()\n",
    "seed = 42\n",
    "initial_lr = 1e-3\n",
    "key = jax.random.PRNGKey(seed)\n",
    "cartpole_env = cartpole.Cartpole(seed=seed)\n",
    "# cartpole_env = CartPole(time_per_step=0.1)\n",
    "# action_space = cartpole_env.action_space\n",
    "action_space = jnp.array([0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect experience (states, actions) and format it as input to the GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy:\n",
    "    def __init__(self, key, action_space):\n",
    "        self.key = key\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def step(self, time_step, mode=None):\n",
    "        return jax.random.choice(key=self.key, a=self.action_space)\n",
    "        # return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = RandomPolicy(rng, action_space)\n",
    "replay_buffer = ReplayBuffer(5000)\n",
    "driver = Driver(\n",
    "    mode=\"random\",\n",
    "    env=cartpole_env,\n",
    "    policy=random_policy,\n",
    "    transition_observers=[replay_buffer.push],\n",
    "    observers=[],\n",
    "    max_steps=30\n",
    ")\n",
    "driver.run(cartpole_env.reset())\n",
    "experience = replay_buffer.gather_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_model_optimizer_callback(epoch, loss_history):\n",
    "    if epoch % 20 == 0:\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=[16, 8])\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Mean ELBO = %.3f\" % -jnp.mean(jnp.array(loss_history[-32:])))\n",
    "        plt.scatter(jnp.arange(len(loss_history)), jnp.array(loss_history)*-1.0)\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['kernel', 'likelihood', 'mean_function', 'inducing_variable', 'q_mu', 'q_sqrt'])\n"
     ]
    }
   ],
   "source": [
    "sgd_optimizer = SGD(\n",
    "    optimizer=optax.adam(initial_lr),\n",
    "    callback=transition_model_optimizer_callback\n",
    ")\n",
    "\n",
    "model_spec = GPModelSpec(\n",
    "    type=SVGP,\n",
    "    num_inducing=32,\n",
    "    likelihood=Gaussian(),\n",
    "    model_uncertainty=True,\n",
    ")\n",
    "\n",
    "transition_model = GPTransitionModel(\n",
    "    gp_model_spec = model_spec,\n",
    "    inference_strategy = None,\n",
    "    optimizer = sgd_optimizer,\n",
    "    reinitialize = True\n",
    ")\n",
    "data = transition_model.get_gp_data(experience)\n",
    "transition_model.initialize(experience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_learning_rate = 1e-3\n",
    "batch_size = 60\n",
    "num_epochs = 900\n",
    "model_params = transition_model.model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = data\n",
    "training_data = CustomDataset(inputs, targets)\n",
    "train_dataloader = NumpyLoader(training_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure transition model's optimizer based on custom loss and training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svgp_transforms = transition_model.model.get_transforms()\n",
    "# constrain_params = build_constrain_params(svgp_transforms)\n",
    "# elbo = transition_model.model.build_elbo(constrain_params=constrain_params, num_data=experience[0].shape[0])\n",
    "\n",
    "# def negative_elbo(params, batch):\n",
    "#     return - elbo(params, batch)\n",
    "\n",
    "# adam = optax.adam(start_learning_rate)\n",
    "\n",
    "# @jax.jit\n",
    "# def train_step(step_i, params, opt_state, batch):\n",
    "#     loss, grads = jax.value_and_grad(negative_elbo, argnums=0)(params, batch)\n",
    "#     updates, opt_state = adam.update(grads, opt_state)\n",
    "#     return loss, updates, opt_state\n",
    "\n",
    "# transition_model.optimizer.set_train_step(train_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ True],\n",
       "             [ True],\n",
       "             [False],\n",
       "             [False],\n",
       "             [False]], dtype=bool)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import jax.numpy as jnp\n",
    "dones = jnp.array([0,0,1,0,0])\n",
    "ep_mask = (jnp.cumsum(dones) < 1).reshape(5, 1)\n",
    "ep_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition_model.train(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_state = adam.init(model_params)\n",
    "# params = model_params\n",
    "# loss_history = []\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     loss, updates, opt_state = train_step(epoch, params, opt_state, data)\n",
    "#     params = optax.apply_updates(params, updates)\n",
    "#     loss_history.append(loss)\n",
    "\n",
    "#     if epoch % 20 == 0:\n",
    "#         clear_output(True)\n",
    "#         plt.figure(figsize=[16, 8])\n",
    "#         plt.subplot(1, 2, 1)\n",
    "#         plt.title(\"Mean ELBO = %.3f\" % -jnp.mean(jnp.array(loss_history[-32:])))\n",
    "#         plt.scatter(jnp.arange(len(loss_history)), jnp.array(loss_history)*-1.0)\n",
    "#         plt.grid()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karyam/Library/Caches/pypoetry/virtualenvs/mbrlax-W83qhuLc-py3.8/lib/python3.8/site-packages/jax/_src/tree_util.py:188: FutureWarning: jax.tree_util.tree_multimap() is deprecated. Please use jax.tree_util.tree_map() instead as a drop-in replacement.\n",
      "  warnings.warn('jax.tree_util.tree_multimap() is deprecated. Please use jax.tree_util.tree_map() '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((30, 6), (30, 6))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, cov = transition_model.model.predict_f(model_params, inputs)\n",
    "mean.shape, cov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 6)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "samples = sample_mvn(rng, mean, cov)\n",
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90, 7), (90, 6))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, targets = transition_model.get_gp_data(experience)\n",
    "inputs.shape, targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_probability.substrates import jax as tfp\n",
    "tfb = tfp.bijectors\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mbrlax.inference_strategy import ConditionalSamplingStrategy\n",
    "from mbrlax.utils import sample_mvn\n",
    "inference_strategy = ConditionalSamplingStrategy(key=key, sampling_strategy=sample_mvn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_term_cost(params, max_steps, time_step):\n",
    "    cost = 0\n",
    "    for step in range(max_steps):\n",
    "        cost += gaussian_objective(time_step.observation)\n",
    "        action = policy.step(time_step, mode=\"plan\")\n",
    "        next_time_step = agent.environment_model.step(action)\n",
    "        time_step = next_time_step\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "policy_optimizer = CMAOptimizer(\n",
    "    key=key,\n",
    "    fitness_function, \n",
    "    num_generations, \n",
    "    pop_size, \n",
    "    num_params, \n",
    "    callback=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'policy_optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xf/9wlqrlfd1dz_z97m_qlvwy6w0000gn/T/ipykernel_3974/3600536260.py\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0maction_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mgp_model_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_model_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0minference_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minference_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'policy_optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "#TODO: cast with default_float\n",
    "invlink = tfb.Chain(bijectors=[\n",
    "    tfb.Scale(scale=20-1e-5),\n",
    "    tfb.Shift(shift=-0.5),\n",
    "    tfb.NormalCDF()]\n",
    ")\n",
    "\n",
    "policy_model_spec = GPModelSpec(\n",
    "    type=SVGP,\n",
    "    num_inducing=32,\n",
    "    likelihood=Gaussian(),\n",
    "    prior=None,\n",
    "    mean_function=\"default\",\n",
    "    model_uncertainty=False,\n",
    "    invlink = invlink\n",
    ")\n",
    "\n",
    "policy = GPPolicy(\n",
    "    action_space = jnp.array([0,1,2]),\n",
    "    gp_model_spec=policy_model_spec,\n",
    "    optimizer=policy_optimizer,\n",
    "    inference_strategy=inference_strategy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_scale = jnp.diag(jnp.array([0.1, 0.1, 0.1, 0.1], dtype=dtype))\n",
    "state_loc = jnp.array([0.0, jnp.pi, 0.0, 0.0], dtype=dtype)\n",
    "initial_state_distribution = tfd.MultivariateNormalTriL(loc=state_loc, scale_tril=state_scale)\n",
    "initial_state_model = ParticleInitialStateModel(initial_state_distribution)\n",
    "initial_obs = initial_state_model.sample(rng, 128)\n",
    "assert(initial_obs.shape == (128, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_model = EnvironmentModel(\n",
    "    transition_model=transition_model,\n",
    "    reward_model=reward_model,\n",
    "    initial_state_model=initial_state_model\n",
    ")\n",
    "\n",
    "virtual_driver = Driver(\n",
    "    mode=\"plan\",\n",
    "    env=environment_model,\n",
    "    policy=policy,\n",
    "    transition_observers=[virtual_replay_buffer.push],\n",
    "    observers=[],\n",
    "    max_steps=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.initialize(experience)\n",
    "virtual_driver.run(environment_model.reset())\n",
    "virtual_experience = virtual_replay_buffer.get_last_n(virtual_driver.max_steps)\n",
    "result = policy.train(virtual_experience)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "39f594e56ca271a51379bd3ff126d91aa5490b0b6c1128ffbd9d181012798abb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('mbrlax-W83qhuLc-py3.8': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
