{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment setting\n",
    "\n",
    "* **Inference**: Moment matching\n",
    "* **Transition model**: SVGP reinitialized during each episode and trained based on all real experience collected so far\n",
    "* **Transition model optimizer**: LBFGS\n",
    "* **Policy** kernel regressor trained with VI, initialized with real experience and trained based on virtual experience\n",
    "* **Policy optimizer**: Adam\n",
    "* **State encoding**: N/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from gpflow.likelihoods import Gaussian\n",
    "from gpflow.optimizers import Scipy\n",
    "from gpflow.config import default_float\n",
    "\n",
    "import gpflow_pilco\n",
    "from gpflow_pilco.envs import CartPole\n",
    "from gpflow_pilco.models.priors import PilcoPenaltySNR\n",
    "from gpflow_pilco.utils.optimizers import GradientDescent\n",
    "from gpflow_pilco.components import GaussianObjective\n",
    "\n",
    "from mbrlax.policy import GPPolicy\n",
    "from mbrlax.transition_model import GPTransitionModel\n",
    "from mbrlax.models import GPModelSpec\n",
    "from mbrlax.agents import PilcoAgent\n",
    "from mbrlax.harness import ExperimentHarness\n",
    "from mbrlax.inference_strategy import MomentMatchingStrategy\n",
    "from mbrlax.utils import MomentsInitialStateModel\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay\n",
    "from tensorflow_probability.python.distributions import MultivariateNormalTriL\n",
    "from tensorflow_probability.python import bijectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.getcwd()\n",
    "cartpole_env = CartPole(time_per_step=0.1)\n",
    "dtype = default_float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_strategy = MomentMatchingStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_model_spec = GPModelSpec(\n",
    "    type = gpflow_pilco.models.SVGP,\n",
    "    num_inducing = 32,\n",
    "    likelihood = Gaussian(),\n",
    "    prior = PilcoPenaltySNR(threshold=1e5, power=30),\n",
    "    mean_function = \"default\",\n",
    "    model_uncertainty = True,\n",
    ")\n",
    "\n",
    "transition_model = GPTransitionModel(\n",
    "    gp_model_spec = transition_model_spec,\n",
    "    inference_strategy = inference_strategy,\n",
    "    optimizer = Scipy(),\n",
    "    reinitialize = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = tf.zeros([4], dtype=default_float())\n",
    "height = cartpole_env.pole.height\n",
    "precis = 16 * tf.convert_to_tensor([[height ** 2, 0, -height, 0, 0],\n",
    "                                    [0, height ** 2, 0, 0, 0],\n",
    "                                    [-height, 0, 1, 0, 0],\n",
    "                                    [0, 0, 0, 0, 0],\n",
    "                                    [0, 0, 0, 0, 0]], dtype=default_float())\n",
    "objective = GaussianObjective(target=target, precis=precis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent optimizer\n",
    "initial_learning_rate = 0.01\n",
    "step_limit = 5000\n",
    "global_clipnorm = 1.0\n",
    "\n",
    "values = tuple((0.1 ** k) * initial_learning_rate for k in range(3))\n",
    "bounds = tuple(k * step_limit // len(values) for k in range(1, len(values)))\n",
    "schedule = PiecewiseConstantDecay(boundaries=bounds, values=values)\n",
    "adam = Adam(learning_rate=schedule, global_clipnorm=global_clipnorm)\n",
    "policy_optimizer = GradientDescent(optimizer=adam, step_limit=step_limit)\n",
    "\n",
    "# inverse link function\n",
    "invlink = bijectors.Chain(bijectors=[\n",
    "    bijectors.Scale(scale=tf.cast(x=20 - 1e-5, dtype=default_float())),\n",
    "    bijectors.Shift(shift=tf.cast(x=-0.5, dtype=default_float())),\n",
    "    bijectors.NormalCDF()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model_spec = GPModelSpec(\n",
    "    type=gpflow_pilco.models.SVGP,\n",
    "    num_inducing=32,\n",
    "    likelihood=Gaussian(),\n",
    "    prior=None,\n",
    "    mean_function=\"default\",\n",
    "    model_uncertainty=False,\n",
    "    invlink = invlink\n",
    ")\n",
    "\n",
    "policy = GPPolicy(\n",
    "    action_space=cartpole_env.action_space,\n",
    "    gp_model_spec=policy_model_spec,\n",
    "    objective=objective,\n",
    "    optimizer=policy_optimizer,\n",
    "    inference_strategy=inference_strategy\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial state distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_scale = tf.linalg.diag(0.1 + tf.zeros([4], dtype=dtype))\n",
    "state_loc = tf.convert_to_tensor(value=(0.0, np.pi, 0.0, 0.0), dtype=dtype)\n",
    "initial_state_distribution = MultivariateNormalTriL(loc=state_loc, scale_tril=state_scale)\n",
    "initial_state_model = MomentsInitialStateModel(initial_state_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilco_agent = PilcoAgent(\n",
    "    transition_model = transition_model,\n",
    "    reward_model = lambda x: None, #explicit rewards not used, instead rely on objective to evaluate policy\n",
    "    initial_state_model = initial_state_model,\n",
    "    policy = policy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Driver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xf/9wlqrlfd1dz_z97m_qlvwy6w0000gn/T/ipykernel_7152/3859058237.py\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmax_eval_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mharness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/mbrlax/mbrlax/harness.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m         )\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         real_collect_driver = Driver(\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mpolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Driver' is not defined"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(\"mm_gradient_descent\")\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    format='%(asctime)-4s %(levelname)s:%(name)s:%(message)s')\n",
    "\n",
    "harness = ExperimentHarness(\n",
    "    logger = logger,\n",
    "    logging_file = open(root_dir + \"/logs/mm_gradient_descent.txt\", \"a\"),\n",
    "    agent = pilco_agent,\n",
    "    env = cartpole_env,\n",
    "    max_train_episodes = 10,\n",
    "    max_eval_episodes = 0,\n",
    ")\n",
    "harness.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "39f594e56ca271a51379bd3ff126d91aa5490b0b6c1128ffbd9d181012798abb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('mbrlax-W83qhuLc-py3.8': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
